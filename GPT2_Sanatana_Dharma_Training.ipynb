{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-2 Sanatana Dharma Training\n",
        "## Fine-tune GPT-2 on Bhagavad-Gita Dataset\n",
        "\n",
        "This notebook trains GPT-2 on the Bhagavad-Gita dataset to create a specialized AI assistant for Hindu scriptures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets accelerate torch scikit-learn numpy tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import (\n",
        "    GPT2Tokenizer, \n",
        "    GPT2LMHeadModel, \n",
        "    TrainingArguments, \n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Upload Training Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the enhanced Bhagavad-Gita dataset\n",
        "print(\"üìÅ Please upload your enhanced_llama_training_dataset.jsonl file\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "dataset_file = None\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.jsonl'):\n",
        "        dataset_file = filename\n",
        "        break\n",
        "\n",
        "if dataset_file:\n",
        "    print(f\"‚úÖ Dataset uploaded: {dataset_file}\")\n",
        "else:\n",
        "    print(\"‚ùå No JSONL file found. Please upload the dataset file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_gita_dataset(jsonl_path):\n",
        "    \"\"\"Load and format the Bhagavad-Gita dataset for GPT-2 training\"\"\"\n",
        "    texts = []\n",
        "    \n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Loading dataset\"):\n",
        "            data = json.loads(line.strip())\n",
        "            \n",
        "            # Format as conversation for GPT-2\n",
        "            if 'instruction' in data and 'output' in data:\n",
        "                # Format: \"Human: {instruction} Assistant: {output}<|endoftext|>\"\n",
        "                text = f\"Human: {data['instruction']} Assistant: {data['output']}<|endoftext|>\"\n",
        "                texts.append(text)\n",
        "            elif 'prompt' in data and 'response' in data:\n",
        "                # Format: \"Question: {prompt} Answer: {response}<|endoftext|>\"\n",
        "                text = f\"Question: {data['prompt']} Answer: {data['response']}<|endoftext|>\"\n",
        "                texts.append(text)\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(texts)} training examples\")\n",
        "    return texts\n",
        "\n",
        "# Load the dataset\n",
        "if dataset_file:\n",
        "    training_texts = load_gita_dataset(dataset_file)\n",
        "    \n",
        "    # Show sample\n",
        "    print(\"\\nüìù Sample training text:\")\n",
        "    print(training_texts[0][:200] + \"...\")\n",
        "else:\n",
        "    print(\"‚ùå No dataset file available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare GPT-2 Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GPT-2 tokenizer\n",
        "print(\"üîÑ Loading GPT-2 tokenizer...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add special tokens for our use case\n",
        "special_tokens = {\n",
        "    \"pad_token\": \"<|pad|>\",\n",
        "    \"eos_token\": \"<|endoftext|>\",\n",
        "    \"bos_token\": \"<|startoftext|>\",\n",
        "    \"sep_token\": \"<|sep|>\"\n",
        "}\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "print(\"‚úÖ Tokenizer loaded and special tokens added\")\n",
        "\n",
        "# Load GPT-2 model\n",
        "print(\"üîÑ Loading GPT-2 model...\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Resize token embeddings for new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "print(\"‚úÖ Model loaded and token embeddings resized\")\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üñ•Ô∏è Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(device)\n",
        "    print(f\"üöÄ Model moved to GPU: {torch.cuda.get_device_name()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tokenize Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the dataset for training\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Create dataset\n",
        "print(\"üîÑ Creating dataset...\")\n",
        "dataset = Dataset.from_dict({\"text\": training_texts})\n",
        "\n",
        "# Tokenize dataset\n",
        "print(\"üîÑ Tokenizing dataset...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Dataset tokenized successfully\")\n",
        "print(f\"üìä Dataset size: {len(tokenized_dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configure Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # GPT-2 is not a masked language model\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_sanatana_dharma\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    remove_unused_columns=False,\n",
        "    fp16=torch.cuda.is_available(),  # Use fp16 if GPU available\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=None,  # Disable wandb\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training configuration set up\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(f\"üìä Training on {len(tokenized_dataset)} examples\")\n",
        "print(f\"‚öôÔ∏è Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"üì¶ Batch size: {training_args.per_device_train_batch_size}\")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"‚úÖ Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save the Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "print(\"üíæ Saving trained model...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(\"./gpt2_sanatana_dharma\")\n",
        "\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "print(\"üìÅ Model saved to: ./gpt2_sanatana_dharma\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test the Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_gpt2_model(prompt, max_length=150):\n",
        "    \"\"\"Test the trained GPT-2 model\"\"\"\n",
        "    \n",
        "    # Format prompt\n",
        "    formatted_prompt = f\"Question: {prompt} Answer:\"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.to(device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract answer part\n",
        "    if \"Answer:\" in response:\n",
        "        answer = response.split(\"Answer:\")[-1].strip()\n",
        "    else:\n",
        "        answer = response\n",
        "    \n",
        "    return answer\n",
        "\n",
        "# Test the model with various questions\n",
        "test_questions = [\n",
        "    \"What is dharma?\",\n",
        "    \"Explain karma yoga\",\n",
        "    \"What does the Bhagavad-Gita teach about detachment?\",\n",
        "    \"How should one perform their duties?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing trained model...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\n‚ùì Question: {question}\")\n",
        "    answer = test_gpt2_model(question)\n",
        "    print(f\"ü§ñ Answer: {answer}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Download the Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a zip file of the trained model\n",
        "import shutil\n",
        "\n",
        "print(\"üì¶ Creating model package...\")\n",
        "shutil.make_archive(\"gpt2_sanatana_dharma_model\", \"zip\", \"./gpt2_sanatana_dharma\")\n",
        "\n",
        "print(\"üì• Downloading trained model...\")\n",
        "files.download(\"gpt2_sanatana_dharma_model.zip\")\n",
        "\n",
        "print(\"‚úÖ Model download initiated!\")\n",
        "print(\"\\nüìã Next steps:\")\n",
        "print(\"1. Download the zip file\")\n",
        "print(\"2. Extract it to your server/models/ directory\")\n",
        "print(\"3. Update your model loader to use the trained GPT-2 model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
